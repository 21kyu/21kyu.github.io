---
title: Scheduling In Go II - Go Scheduler
author: wq
name: Wongyu Lee
link: https://github.com/kyu21
date: 2022-04-19 22:00:00 +0900
categories: [go]
tags: [go, scheduler, os]
render_with_liquid: false
---

> [Scheduling In Go : Part II - Go Scheduler](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html)
를 옮긴 글

## Prelude

Go의 스케줄러 내부가 돌아가는 메커니즘(mechanics)과 의미(semantics)에 대한 이해를 제공할 3부작 시리즈의 첫번째 게시물이다.
첫번째 게시물은 운영체제 스케줄러에 중점을 둔다.

**3부작**:
1. [Scheduling In Go : Part I - OS Scheduler](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html)
2. [Scheduling In Go : Part II - Go Scheduler](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html)
3. [Scheduling In Go : Part III - Concurrency](https://www.ardanlabs.com/blog/2018/12/scheduling-in-go-part3.html)

## Introduction

스케줄링 시리즈의 첫 파트에서 Go 스케줄러의 구조를 이해하고 인지하는데 중요하다고 생각되는 운영체제의 스케줄러 관점에서 설명했다.
이번 게시글에서는 Go 스케줄러가 동작하는 방식을 의미론적 수준에서 설명하고, 고수준의 동작에 초점을 맞출 것이다.
Go 스케줄러는 복잡한 시스템이며 아주 조그만 메카니컬한 세부사항은 보다는 그것이 어떻게 동작하고 행동하는지에 대한 좋은 시야를 가지는 것이 중요하다.
이는 당신이 더 나은 엔지니어링 결정을 하도록 도와줄 것이다.

## Your Program Starts

Go 프로그램이 시작될 때, 호스트 머신에서 식별되는 모든 가상 코어(virtual core)에 대해 논리 프로세서(Logical Processor) `P`가 제공된다.
물리적 코어당 여러 개의 하드웨어 스레드(hardware thread)가 있는 프로세서(Hyper-Threading[^1])가 있는 경우,
각 하드웨어 스레드는 Go 프로그램에서 가상 코어로 표시된다.
더 나은 이해를 위해 한 맥북 프로 모델의 시스템 리포트를 살펴보자.

![hardware overview](/images/hardware-overview.png)
_그림 1_

4개의 물리적 코어가 있는 단일 프로세서가 있음을 확인할 수 있다.
이 리포트에서는 물리적 코어 당 하드웨어 스레드의 수를 보여주지 않지만 이 맥북 프로가 보유한
Intel Core i7 프로세서에는 물리적 코어 당 2개의 하드웨어 스레드를 가지는 하이퍼 스레딩 기능이 있다.
운영체제 스레드를 병렬로 실행하려 할 때, 8개의 가상 코어를 사용할 수 있다는것을 Go 프로그램이 알게 된다.

이를 테스트하기 위한 아래의 프로그램을 보자:

```go
package main

import (
	"fmt"
	"runtime"
)

func main() {

    // NumCPU returns the number of logical
    // CPUs usable by the current process.
    fmt.Println(runtime.NumCPU())
}
```
{: .nolineno }

그림 1.의 머신에서 이 프로그램을 실행하면 `NumCPU()` 함수 호출의 결과는 8이 될 것이다.
해당 머신에서 실행되는 모든 Go 프로그램에는 8개의 `P`가 주어진다.
모든 `P`에는 운영체제 스레드 `M`이 할당된다.
이 스레드는 여전히 운영체제에 의해 관리되며 지난 게시글에서 설명한바와 같이 운영체제는 실행을 위해 코어에 스레드를 배치할 책임이 있다.
즉, 위 머신에서 Go 프로그램을 실행할 때 작업을 수행하는데 사용가능한 8개의 스레드가 존재하며 각각 `P`에 개별적으로 연결된다.

모든 Go 프로그램에는 Go 프로그램의 실행 경로인 초기 고루틴(initial Goroutine) `G`가 주어진다.
고루틴은 본질적으로 Coroutine[^2]이지만 여기는 Go 세상이므로, 문자 C를 G로 바꾼 고루틴이라 불리게 된다.
고루틴은 애플리케이션 수준의 스레드로 생각할 수 있으며 여러 면에서 운영체제의 스레드와 유사하다.
운영체제 스레드는 컨텍스트 전환을 각 코어에서 하지만, 고루틴은 운영체제 스레드 `M`에서 컨텍스트 전환이 이루어 진다.

퍼즐의 마지막 조각은 실행 큐(run queue)이다.
Go 스케줄러에는 전역 실행 큐(Global Run Queue, `GRQ`)와 로컬 실행 큐(Local Run Queue,`LRQ`)라는 두 가지의 실행 큐가 있다.
각 `P`에는 `P`의 컨텍스트 내에서 실행되도록 할당된 고루틴을 관리하는 `LRQ`가 주어진다.
`LRQ` 내 고루틴들은 해당 `P`에 할당된 `M`에서 컨텍스트 전환되며 각자의 작업을 위한 시간을 갖는다.
`GRQ`는 아직 `P`에 할당되지 않은 고루틴들을 위한 것으로 `GRQ`에서 `LRQ`로 이동하는 고루틴들의 프로세스는 이후에 논의될 것이다.

그림 2.는 이 모든 구성 요소의 대한 전체적인 이미지를 제공한다.

![goroutine components](/images/goroutine-components.png)
_그림 2_

## Cooperating Scheduler

앞선 게시글에서 의논했던 바와 같이 운영체제 스케줄러는 선점형 스케줄러이며 이는 스케줄러가 특정 주어진 시간에 무엇을 할지 예상할 수 없다는 것을 의미한다.
커널이 내리는 결정은 모두 비결정적이다.
운영제체에서 실행되는 애플리케이션들은 원자적(atomic[^3]) 명령어 및 뮤텍스(mutex[^4]) 호출과 같은 동기화 요소들을 활용하지 않는 한
스케줄링을 통해 커널에서 내부에서 일어날 일들을 제어할 수 없다.

Go 스케줄러는 Go 런타임의 일부이며, Go 런타임은 애플리케이션에 내장되어 있다.
이는 Go 스케줄러가 커널 위의 사용자 영역[^5]에서 실행된다는 것을 의미한다.
현재 Go 스케줄러의 구현은 선점형 스케줄러가 아니라 협조적(cooperating[^6]) 스케줄러이며,
이러한 협조적 스케줄러는 스케줄링 결정을 내리기 위해 코드의 안전한 지점에서 발생하는 잘 정의된 사용자 영역의 이벤트가 필요하다.
Go 협조적 스케줄러의 놀라운 점은 선점형처럼 동작한다는 것인데, 사용자는 Go 스케줄러가 무엇을 할 지 예측하기가 어렵다.
이 협력형 스케줄러에 대한 의사 결정은 개발자가 아니라 Go 런타임이 하기 때문이다.
이렇듯 Go 스케줄러를 선점형 스케줄러로 생각하는 것이 중요하며 스케줄러는 비결정적이기 때문에 크게 확장되지 않는다.

## Goroutine States

고루틴은 스레드와 동일한 3가지 형태의 고수준 상태를 가진다.
고루틴이 수행하게될 역할을 Go 스케줄러가 지시하게 되며, 고루틴은 다음 3개의 상태 중 하나가 될 수 있다:

Waiting
: 고루틴은 중지되었으며 작업이 계속되기 위한 무언가를 기다리고 있음을 의미한다.
이는 운영체제 시스템 콜이나 atomic 및 mutex 연산과 같은 동기화 호출을 기다리는 것과 같은 이유로 발생될 수 있다.
이러한 유형의 대기 시간은 성능 저하의 근본 원인이 된다.

Runnable
: 고루틴이 할당된 명령어들을 실행할 수 있도록 `M`에 배치되기를 원하는 상태이다.
배치되기를 원하는 고루틴들이 많이 있으면, 더 오랜 시간을 기다려야 한다.
또한 각 고루틴에게 주어질 시간의 양은 더 많은 고루틴들이 경쟁을 할수록 짧아진다.
이 유형의 대기 시간 또한 성능 저하의 원인이 된다.

Executing
: 고루틴이 `M`에 배치됐고 명령어를 실행하고 있음을 의미한다.
애플리케이션과 관련된 작업이 수행되며 모두가 원하는 것 이다.

## Context Switching

Go 스케줄러는 컨텍스트 전환을 위한 코드 내 안전한 지점에서 발생할 잘 정의된 사용자 영역의 이벤트가 필요하다.
이러한 이벤트와 안전한 지점은 함수 호출 내에서 나타난다.
함수 호출은 Go 스케줄러의 상태에 매우 중요한 요인으로, 오늘날 (1.11 버전 이하의 Go)
함수 호출을 하지 않는 tight loop[^7]를 실행하면 스케줄러 및 가비지 컬렉션 내에서의 지연 시간이 발생된다.
따라서 함수 호출이 합리적인 주기 내에서 발생되는 것이 매우 중요하다.

> Node: tight loop의 선점을 허용하기 위한 Go 스케줄러 내부의 비협조적 선점 기술을 적용하기 위해 승인된 1.12 버전에 대한 [제안](https://github.com/golang/go/issues/24543) 이 있다.

Go 프로그램에는 스케줄러의 스케줄링 결정이 발생하도록 하는 4가지 이벤트가 존재한다.

The use of the keyword go
: `go` 키워드는 고루틴을 생성하는 방법이다.
새로운 고루틴이 생성되면 스케줄러에게 스케줄링 결정을 내릴 수 있는 기회를 제공한다.

Garbage collection
: GC는 자체 고루틴 세트를 사용해 실행되기 때문에, 이러한 고루틴들은 `M`에 배치된 후 실행되어야 한다.
이로 인해 GC는 스케줄링의 많은 혼란을 야기하지만 스케줄러는 고루틴이 하는 작업에 대한 해박한 지식이 있으며 현명한 스케줄링 결정을 내리기 위해 해당 지식을 십분 활용할 것이다.
이러한 현명한 스케줄링 결정 중 하나는 GC가 발생되는 동안 힙을 건드리지 않는 고루틴과 힙을 건드리려는 고루틴의 컨텍스트 전환이다.
GC가 실행될 때에는 수많은 스케줄링 결정이 이루어지게 된다.

System calls
: 고루틴이 `M`이 차단되도록 만드는 시스템 콜을 발생시키게 되면
스케줄러는 해당 고루틴을 `M`에서 내리고 동일한 `M`으로 새로운 고루틴을 배치할 수 있다.
그러나, `P`에서 대기 중인 고루틴을 실행하기 위한 새로운 `M`이 필요한 경우도 있다.
이것이 어떻게 작동하는지에 대해 다음 섹션에서 더 자세히 설명하겠다.

Synchronization and Orchestration
: 아토믹, 뮤텍스 또는 채널 작업의 호출로 인해 고루틴이 차단되면 스케줄러는 새로운 고루틴이 실행되도록 컨텍스트 전환 할 수 있다.
추후에 차단된 고루틴이 실행될 수 있을 때 큐에 다시 들어가 결과적으로는 컨텍스트 전환을 통해 다시 `M`에 배치될 것이다.

## Asynchronous System Calls

비동기 시스템 콜을 처리할 수 있는 능력을 가진 운영체제에서는 [네트워크 폴러](https://go.dev/src/runtime/netpoll.go) 라 불리는 것을 사용해 시스템 호출을 보다 효율적으로 처리할 수 있다.
이는 해당 운영체제 내에서 kqueue(MacOS), epoll(Linux) 또는 iocp(Windows)를 사용하여 수행된다.

네트워킹 기반 시스템 콜은 오늘날 우리가 사용하고 있는 많은 운영체제에서 비동기적으로 처리될 수 있다.
네트워크 폴러는 네트워킹 작업을 처리하는 것이 주 용도라서 그런 이름을 가지게 되었다.
네트워크 폴러를 통해 네트워킹 시스템 호출을 사용하게 되면 스케줄러는 해당 시스템 호출로 인해 고루틴이 `M`이 차단되도록 하는 것을 방지할 수 있다.
이 동작은 새로운 `M`을 생성할 필요 없이 `P`의 `LRQ`에 있는 다른 고루틴이 실행 가능하도록 `M`을 유지하며 이는 운영체제의 스케줄링 부하를 줄이는 데 도움이 된다.

예제를 살펴보자.

![net poller 1](/images/net-poller-1.png)
_그림 3_

그림 3은 기본 스케줄링 다이어그램을 보여준다.
`G1`은 `M`에서 실행되고 있으며 3개 이상의 고루틴들이 `M`에 배치되기 위해 `LRQ`에서 대기하고 있다.
`Net Poller`는 아무런 작업도 수행하지 않는 유휴(idle) 상태이다.

![net poller 2](/images/net-poller-2.png)
_그림 4_

그림 4에서 `G1`은 네트워크 시스템 콜을 수행하길 원하여 `Net Poller`로 옮겨졌으며 비동기 네트워크 시스템 호출이 처리되고 있다.
`G1`이 `Net Poller`로 옮겨질 때, `M`은 `LRQ`의 다른 고루틴을 실행할 수 있게 된다.
이 경우에서는 `G2`가 컨텍스트 전환되어 `M`에 배치되었다.

![net poller 3](/images/net-poller-3.png)
_그림 5_

그림 5에서는 비동기 네트워크 시스템 콜이 `Net Poller`에서 처리 완료됐으며 `G1`은 `P`의 `LRQ`로 다시 옮겨졌다.
이후에 `G1`이 컨텍스트 전환돼 `M`에 다시 배치되면 `G1`이 담당하는 Go 관련 코드가 다시 실행될 것이다.
여기에서의 가장 큰 이점이라고 한다면 네트워크 시스템 호출이 실행될 때 추가적인 `M`이 필요하지 않다는 점이다.
`Net Poller`는 하나의 운영체제 스레드를 가졌으며 효율적으로 돌아가는 이벤트 루프를 처리하고 있다.

## Synchronous System Calls

고루틴이 비동기적으로 동작할 수 없는 시스템 콜을 호출하길 원하면 어떻게 될까?
이 경우 네트워크 폴러는 사용될 수 없으며 시스템 콜을 호출하는 고루틴은 `M`이 차단되게 한다.
불행히도 이 동직이 발생되지 않도록 막을 방법은 없다.
비동기적으로 호출될 수 없는 시스템 콜의 한 예는 파일 기반 시스템 콜이다.
CGO를 사용 중이라면, C 함수를 호출할 때 `M` 또한 차단되는 상황이 있을 수도 있다.

> Note: Windows 운영체제는 파일 기반 시스템 콜을 비동기적으로 호출할 수 있다.
기술적으로 Windows에서 실행할 때에는 네트워크 폴러를 사용할 수 있다.

`M`이 차단되게 만드는 file I/O와 같은 동기적으로 동작하는 시스템 호출에서는 어떠한 일이 발생되는지 살펴보자.

![synchronous system calls 1](/images/synchronous-system-calls-1.png)
_그림 6_

그림 6은 그림 3에 이어 한번 더 기본 스케줄링 다이어그램을 보여주지만 이번에는 `G1`이 `M1`이 차단될 동기적 시스템 콜을 호출할 것이다.

![synchronous system calls 2](/images/synchronous-system-calls-2.png)
_그림 7_

그림 7에서 스케줄러는 `G1`에 의해 `M1`이 차단됐음을 식별할 수 있다.
이 시점에서 스케줄러는 `M1`을 `G1`이 여전히 연결된 채로 `P`에서 분리한다.
그러면 스케줄러는 `P`가 계속 작업을 할 수 있도록 새로운 `M2`를 가져온다.
`LRQ`에서 `G2`가 컨텍스트 전한되어 `M2`에 배치된다.
만약 이전에 발생됐던 위와 같은 스왑으로 인해 다른 `M`이 이미 존재하는 경우, 이 전환 작업은 새로운 `M`을 생성하는 것보다 빠를 것이다.

![synchronous system calls 3](/images/synchronous-system-calls-3.png)
_그림 8_

그림 8에서는 `G1`에 의해 발생된 blocking 시스템 콜이 종료된다.
이 시점에서는 `G1`은 다시 `LRQ`에 들어가며 `P`에서 다시 작업을 이어할 수 있게 된다.
`M1`은 해당 시나리오가 다시 발생될 때 사용할 수 있도록 사이드에 배치된다.

## Work Stealing

스케줄러의 또 다른 측면은 작업을 훔치는 스케줄러라는 것이다.
이는 스케줄링을 효율적으로 유지하는 데 몇 가지 영역에서 도움이 된다.
예를 들어, `M`이 `waiting` 상태로 전환되면 운영체제가 `M`을 코어에서 컨텍스트 전환하기 때문에 `M`이 `waiting` 상태로 전환되는 것은 바람직하지 않다.
즉, `runnable` 상태의 고루틴이 있더라도 `M`이 코어에서 컨텍스트 전환될 때까지 `P`는 어떠한 작업도 완료할 수 없음을 의미한다.
작업 훔치기는 모든 `P`에 걸쳐 고루틴의 균형을 유지하여 작업이 더 잘 분산되고 더 효율적으로 수행되도록 도와준다.

예제를 살펴보자.

![work stealing 1](/images/work-stealing-1.png)
_그림 9_

그림 9는 각각 4개의 고루틴을 포함하는 2개의 `P`와 `GRQ`에 있는 하나의 고루틴을 가진 멀티스레드 Go 프로그램을 나타낸다.
2개의 `P` 중 하나가 모든 고루틴을 빠르게 처리하면 어떤 일이 발생될까?

![work stealing 2](/images/work-stealing-2.png)
_그림 10_

그림 10에서 `P1`에는 더 이상 실행할 고루틴이 없다.
하지만 `P2`의 `LRQ`에도, 외부의 `GRQ`에도 `runnable` 상태의 고루틴들이 기다리고 있다.
여유가 생긴 `P1`이 작업을 훔칠 필요가 있는 순간이라고 할 수 있겠다.
작업 훔치기(stealing work[^8])의 규칙은 다음과 같다.

```go
runtime.schedule() {
    // only 1/61 of the time, check the global runnable queue for a G.
    // if not found, check the local queue.
    // if not found,
    //     try to steal from other Ps.
    //     if not, check the global runnable queue.
    //     if not found, poll network.
}
```
{: .nolineno }

> wq: 위의 stealing work 규칙과는 실제 코드 구현이 조금 달라보여서 추가로 정리해보고자 한다.
{: .prompt-info }

1. 공정성을 보장하기 위해 스케줄러는 가끔(`schedtick%61 == 0`일 때 마다) `GRQ`를 확인한다. 그렇지 않으면 두 고루틴이 서로를 지속적으로 재호출하여 LRQ를 완전히 차지하게 된다.
2. `GRQ`에서 찾은게 없다면 `LRQ`에서 고루틴을 가져온다.
3. `LRQ`에서도 찾은게 없다면, 싫행 가능한 고루틴을 찾을 때까지 이후 작업은 차단된다.
   1. `LRQ`를 확인한다.
   2. `GRQ`를 확인한다.
   3. 네트워크를 폴링한다. 네트워크 폴러로부터 실행 가능한 고루틴을 가져온다.
   4. 다른 `P`에서 작업을 훔쳐온다. 예를 들어 `P2`의 `LRQ`에 있는 고루틴의 절반을 가져와서 `P1`의 `LRQ`에 넣는다.
   5. 아직까지도 고루틴을 찾지 못한거면 할 일이 없는거다. 그럼에도 `P`를 그냥 포기하는 것은 아쉬우니 GC 마크 단계에 있다면 유휴 시간 마킹이라도 한다.
   6. wasm일 경우, 콜백이 반환되었지만 다른 고루틴이 깨어 있지 않다면 콜백이 트리거될 때까지 실행을 일시 중지하는 이벤트 핸들러 고루틴을 깨운다.

위의 룰에 따라, `P1`은 `P2`의 LRQ에서 고루틴을 확인하고 찾은 것의 절반을 가져와야 한다.

> wq: 이 부분에서 설명해주는 동작이 최신 코드 구현과는 다르므로 감안해서 보도록 하자.
{: .prompt-warning }

![work stealing 3](/images/work-stealing-3.png)
_그림 11_

`P1`은 `P2`의 `LRQ`로부터 절반의 고루틴을 가져와서 실행한다.
만약 `P2`가 모든 고루틴의 실행을 마치고 `P1`의 `LRQ`에도 고루틴이 없다면 어떻게 될까?

![work stealing 4](/images/work-stealing-4.png)
_그림 12_

그림 12에서 `P2`는 모든 작업을 마쳤으며 고루틴 몇개를 훔칠 필요성이 생겼다.
우선 `P1`의 `LRQ`를 살펴보지만 가져올 고루틴은 보이지 않는다.
다음 순으로 `GRQ`를 확인한다. 여기에서 고루틴 `G9`을 찾을 수 있다.

![work stealing 5](/images/work-stealing-5.png)
_그림 13_

그림 13에서는 `P2`는 `GRQ`의 `G9`을 훔쳐오고 해당 고루틴의 작업을 실행한다.
이러한 작업 훔치기의 장점은 `M` 모두가 바쁘게 지내며 유휴 상태가 되지 않도록 해준다는 것이다.
작업 훔치기는 내부적으로는 `M`을 회전(spinning)시키는 것으로 간주된다.
이 회전에서는 JBD가 작성한 [블로그 게시물](https://rakyll.org/scheduler/)에서 또 다른 이점에 대해 잘 설명해주고 있다.

## Practical Example

메커니즘과 의미에 대한 이해를 갖춘 상태에서 이 모든 것이 결합되어 Go 스케줄러가 시간이 지남에 따라
더 많은 작업을 수행할 수 있도록 하는 방법을 보여주고자 한다.
두 개의 운영체제 스레드를 사용하여 프로그램이 서로 메시지를 주고 받도록 하는 C로 작성된 멀티스레드 애플리케이션을 떠올려보자.

![practical example 1](/images/practical-example-1.png)
_그림 14_

그림 14에는 메시지를 앞뒤로 전달하는 2개의 스레드가 있다.
`Thread 1`이 `Code 1`에서 컨텍스트 전환되어 실행 중이므로 `Thread 1`이 `Thread 2`로 메시지를 보낼 수 있다.

> Node: 메시지가 전달되는 방식은 중요하지 않다. 중요한 것은 이 작업들이 진행되는 동안의 스레드들의 상태이다.

![practical example 2](/images/practical-example-2.png)
_그림 15_

그림 15에서 `Thread 1`이 메시지 전송을 마치면 이제 응답을 기다려야 한다.
이렇게 되면 `Thread 1`은 컨텍스트 전환되어 `Core 1`에서 내려간 후 `waiting` 상태가 된다.
`Thread 2`가 메시지를 전달받으면 `runnable` 상태가 되고, 운영체제는 컨텍스트 전환을 수행해 `Core 2`에서 `Thread 2`를 실행할 수 있다.
그런 다음 `Thread 2`는 메시지를 처리하고 새로운 메시지를 `Thread 1`에게 다시 보낸다.

![practical example 3](/images/practical-example-3.png)
_그림 16_

그림 16에서 `Thread 2`의 메시지가 `Thread 1`에 수신됨에 따라 스레드는 컨텍스트 전환을 다시 한 번 수행한다.
이제 `Thread 2`는 `executing` 상태에서 `waiting`로, `Thread 1`은 `waiting` 상태에서 `runnable` 상태로 컨텍스트 전환되고
최종적으로는 `executing` 상태로 다시 전환되어서 새 메시지를 처리하고 보낼 수 있게 된다.

이러한 모든 컨텍스트 전환과 상태 변경에는 작업을 빠르게 완료하기 위한 제한된 수행 시간이 필요하다.
각 컨텍스트 전환은 최대 1000 나노초의 지연 시간이 발생될 수 있으며 하드웨어가 나노초 당 12개의 명령어를 수행하기를 기대하는 상황에서는
대략 1,200개의 명령어가 컨텍스트 전환 중에는 실행되지 못한다.
이러한 스레드들도 서로 다른 코어에 배치되기 때문에 캐시 라인 누락으로 인한 추가 지연 시간이 발생될 가능성도 높다.

이제 고루틴과 Go 스케줄러를 사용하는 동일한 예제를 살펴보자.

![practical example 4](/images/practical-example-4.png)
_그림 17_

그림 17엔 서로 오케스트레이션되어 메시지를 주고 받는 두 개의 고루틴이 있다.
`G1`은 자신의 작업을 수행하기 위해 `Core 1`에 배치되어 있는 `M1`에서 컨텍스트 전환되며 `G2`로 메시지를 전달한다.

![practical example 5](/images/practical-example-5.png)
_그림 18_

그림 18에서의 `G1` 또한 메시지 전송을 마치면 이제 응답을 기다려야 한다.
`G1`은 컨텍스트 전환되어 `M1`에서 내려가고 `waiting` 상태가 된다.
`G2`가 `G1`이 보낸 메시지를 전달받으면 `runnable` 상태가 된다.
이제 Go 스케줄러는 컨텍스트 전환을 수행해 `G2`가 `Core 1`에 여전히 배치되어 있는 `M1`에서 실행되게 한다.
그런 다음 `G2`는 메시지를 처리하고 새로운 메시지를 `G1`으로 전달한다.

![practical example 6](/images/practical-example-6.png)
_그림 19_

그림 19에서는 `G2`에서 보낸 메시지가 `G1`에 전달됨에 따라 다시 한번 컨텍스트 전환된다.
이제 `G2`는 `executing` 상태에서 `waiting` 상태로, `G1`은 `waiting` 상태에서 `runnable` 상태로 전환된 후
최종적으로는 전달받은 메시지를 처리하고 보낼 `executing` 상태로 컨텍스트 전환된다.

표면적으로는 별반 달라진게 없다.
스레드를 사용하든 고루틴을 사용하든 컨텍스트 전환과 상태 변경은 발생될 수 밖에 없다.
그러나 스레드와 고루틴의 사용 간에는 언뜻 보기엔 명백하지 않은 큰 차이가 존재한다.

고루틴을 사용하는 경우에 모든 처리에 동일한 운영체제 스레드와 코어를 사용한다.
이는 운영체제 관점에서 운영체제 스레드는 한 순간이라도 `waiting` 상태가 되지 않는다는 것을 의미한다.
그 결과 스레드를 사용했더라면 컨텍스트 전환으로 인해 손실될 모든 명령어들은 고루틴을 사용할 때는 손실되지 않는다.

본질적으로 Go는 IO/Blocking 작업을 운영체제 수준에서 CPU-bound 작업으로 전환한다.
모든 컨텍스트 전환이 애플리케이션 수준에서 발생되기 때문에, 스레드를 사용할 때 손실됐던 컨텍스트 전환 시의 최대 12,000개의 명령어는 더이상 손실되지 않는다.
Go에서는 이런 동일한 컨텍스트 전환의 비용으로 최대 200 나노초 혹은 2,400개의 명령가 소요된다.
스케줄러 또한 효율적인 캐시 라인 효율성과 NUMA를 향상시키는 데도 도움이 된다.
이것이 우리가 가상 코어보다 더 많은 스레드를 필요로 하지 않는 이유다.
Go 스케줄러가 더 적은 스레드를 사용하고 각 스레드에서 더 많은 작업을 수행하고자 시도하므로 시간이 지남에 따라
더 많은 작업을 완료할 수 있으며, 이는 운영체제와 하드웨어의 부하를 줄인다.

## Conclusion

Go 스케줄러는 운영체제와 하드웨어의 작동 방식의 복잡함을 고려하여 설계되었다는 점에서 진심으로 놀라울 따음이다.
운영체제 수준에서 IO/Blocking 작업을 CPU-bound 작업으로 전환하는 기능은 시간이 지남에 따라 더 높은 CPU 처리량을 활용하는 데 큰 도움이 된다.
다시 한번 말하지만, 가상 코어보다 더 많은 스레드가 필요하지 않는 이유이며
가상 코어당 단 하나의 운영체제 스레드만으로 모든 작업(CPU 및 IO/Blocking bound)을 수행할 수 있다.
운영체제 스레드를 차단하는 시스템 호출을 필요로 하지 않는 네트워킹 앱 및 기다 앱의 경우 이러한 작업이 가능해진다.

개발자는 처리 중인 작업의 종류와 관련해 앱이 수행하는 작업을 이해해야 한다.
무한정 많은 고루틴을 만들어서는 놀라운 성능을 기대할 수 없을 것이다.
`Less is always more`, 이러한 Go 스케줄러의 의미를 이해한다면 더 나은 엔지니어링 결정을 내릴 수 있다.
다음 게시물에서는 코드에 추가해야 하는 복잡성의 균형을 유지하면서, 더 나은 성능을 얻기 위해 보수적인 방식으로 동시성을 활용하는 아이디어를 살펴보도록 하겠다.

---

[^1]: [Hyper-threading: wikipedia](https://ko.wikipedia.org/wiki/하이퍼스레딩https://ko.wikipedia.org/wiki/하이퍼스레딩)
[^2]: [Coroutine: wikipedia](https://ko.wikipedia.org/wiki/코루틴)
[^3]: [Linearizability: wikipedia](https://ko.wikipedia.org/wiki/선형화가능성)
[^4]: [Lock: wikipedia](https://ko.wikipedia.org/wiki/락_(컴퓨터_과학))
[^5]: [User space: wikipedia](https://ko.wikipedia.org/wiki/사용자_공간)
[^6]: [Cooperative multitasking: wikipedia](https://en.wikipedia.org/wiki/Cooperative_multitasking)
[^7]: [tight loop: wikipedia](https://en.wiktionary.org/wiki/tight_loop)
[^8]: [Source file src/runtime/proc.go](https://go.dev/src/runtime/proc.go)
